{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8329ea3",
   "metadata": {},
   "source": [
    "In this notebook will be developed a `process_row` function that will be used in the Processor component to process rows from the apartment dataset in Poland. The function will transform each record by applying various cleaning, normalization, and transformation operations.\n",
    "\n",
    "## Processing Goals:\n",
    "\n",
    "1. Converting categorical features to numerical\n",
    "2. Normalizing numerical features to the range [0, 1]\n",
    "3. Filling missing values\n",
    "4. Creating new informative features\n",
    "5. Removing unnecessary features\n",
    "\n",
    "## Function Requirements:\n",
    "- Accepts a single data row (`pandas.Series`)\n",
    "- Returns a processed data row (`pandas.Series`)\n",
    "- Does not modify the original dataset\n",
    "- Converts string binary features ('yes'/'no') to boolean (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba887369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Chart display settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Display all DataFrame columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba52de",
   "metadata": {},
   "source": [
    "Let's load the cleaned dataset from the `cleaned_apartments.csv` file and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data file\n",
    "file_path = '../datasets/cleaned_apartments.csv'\n",
    "\n",
    "# Loading data\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset successfully loaded, size: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {file_path} not found!\")\n",
    "    file_path = '../datasets/apartments.csv'  # Trying to load the original dataset\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Original dataset loaded, size: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} also not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6197236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check information about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d613d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistics for numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values_percent = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "print(\"Number of missing values by column:\")\n",
    "for col, miss_count in sorted(zip(missing_values.index, missing_values), key=lambda x: x[1], reverse=True):\n",
    "    if miss_count > 0:\n",
    "        print(f\"{col}: {miss_count} ({missing_values_percent[col]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6bf947",
   "metadata": {},
   "source": [
    "Let's check what values the binary columns that need to be transformed contain:\n",
    "- hasParkingSpace\n",
    "- hasBalcony\n",
    "- hasElevator\n",
    "- hasSecurity\n",
    "- hasStorageRoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in binary columns\n",
    "binary_columns = ['hasParkingSpace', 'hasBalcony', 'hasElevator', 'hasSecurity', 'hasStorageRoom']\n",
    "\n",
    "for col in binary_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd40162",
   "metadata": {},
   "source": [
    "Let's check what values the categorical columns that will be transformed contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22830680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical features\n",
    "categorical_columns = ['type', 'ownership', 'condition', 'city']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].unique()}\")\n",
    "        print(f\"Number of unique values: {df[col].nunique()}\")\n",
    "        print(f\"Value frequency:\\n{df[col].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845f2fe",
   "metadata": {},
   "source": [
    "Now will be developed the `process_row` function that will perform the following operations:\n",
    "\n",
    "1. Converting string binary features to boolean values\n",
    "2. Filling missing values\n",
    "3. Converting categorical features to numerical\n",
    "4. Normalizing numerical features\n",
    "5. Creating new features\n",
    "6. Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Takes a row from a pandas DataFrame and returns an updated row\n",
    "    with cleaned and transformed data.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): Row from DataFrame with apartment data\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Processed row with transformed data\n",
    "    \"\"\"\n",
    "    # Create a copy of the row to avoid modifying the original\n",
    "    processed = row.copy()\n",
    "    \n",
    "    # 1. Converting string binary features to boolean values\n",
    "    binary_columns = ['hasParkingSpace', 'hasBalcony', 'hasElevator', 'hasSecurity', 'hasStorageRoom']\n",
    "    for col in binary_columns:\n",
    "        if col in processed:\n",
    "            # Fill missing values\n",
    "            if pd.isna(processed[col]):\n",
    "                processed[col] = 'no'  # By default, we assume the feature is absent\n",
    "                \n",
    "                # For hasElevator, we determine by building type and number of floors\n",
    "                if col == 'hasElevator' and 'floorCount' in processed and 'type' in processed:\n",
    "                    if (not pd.isna(processed['floorCount']) and processed['floorCount'] > 4) or \\\n",
    "                       (not pd.isna(processed['type']) and processed['type'] == 'blockOfFlats'):\n",
    "                        processed[col] = 'yes'\n",
    "            \n",
    "            # Convert yes/no to True/False\n",
    "            processed[col] = True if processed[col] == 'yes' else False\n",
    "    \n",
    "    # 2. Fill missing values in numeric columns\n",
    "    numeric_columns = {\n",
    "        'floor': 2,              # Median value\n",
    "        'floorCount': 5,         # Median value\n",
    "        'squareMeters': 50,      # Typical apartment size\n",
    "        'rooms': 2,              # Typical number of rooms\n",
    "        'centreDistance': 5.0,   # Typical distance from center\n",
    "        'poiCount': 10           # Average number of POIs\n",
    "    }\n",
    "    \n",
    "    for col, default_value in numeric_columns.items():\n",
    "        if col in processed and pd.isna(processed[col]):\n",
    "            processed[col] = default_value\n",
    "    \n",
    "    # 3. Convert categorical features to numerical\n",
    "    # Building type\n",
    "    if 'type' in processed:\n",
    "        type_mapping = {\n",
    "            'blockOfFlats': 0,\n",
    "            'tenement': 1,\n",
    "            'apartmentBuilding': 2\n",
    "        }\n",
    "        if not pd.isna(processed['type']):\n",
    "            processed['type_numeric'] = type_mapping.get(processed['type'], 3)\n",
    "        else:\n",
    "            processed['type_numeric'] = 0  # Default value\n",
    "        \n",
    "        # Remove original column\n",
    "        processed = processed.drop('type')\n",
    "    \n",
    "    # Apartment condition\n",
    "    if 'condition' in processed:\n",
    "        condition_mapping = {\n",
    "            'very good': 4,\n",
    "            'good': 3,\n",
    "            'average': 2,\n",
    "            'poor': 1,\n",
    "            'to renovation': 0\n",
    "        }\n",
    "        if not pd.isna(processed['condition']):\n",
    "            processed['condition_numeric'] = condition_mapping.get(processed['condition'], 2)\n",
    "        else:\n",
    "            processed['condition_numeric'] = 2  # Average condition by default\n",
    "        \n",
    "        # Remove original column\n",
    "        processed = processed.drop('condition')\n",
    "    \n",
    "    # 4. Create new features\n",
    "    # Floor ratio to total floors\n",
    "    if 'floor' in processed and 'floorCount' in processed and processed['floorCount'] > 0:\n",
    "        processed['floor_ratio'] = round(processed['floor'] / processed['floorCount'], 3)\n",
    "    else:\n",
    "        processed['floor_ratio'] = 0.5  # Default value\n",
    "    \n",
    "    # Price per square meter\n",
    "    if 'price' in processed and 'squareMeters' in processed and processed['squareMeters'] > 0:\n",
    "        processed['price_per_m2'] = round(processed['price'] / processed['squareMeters'], 2)\n",
    "    \n",
    "    # Combined comfort score\n",
    "    comfort_features = ['hasParkingSpace', 'hasBalcony', 'hasElevator', 'hasSecurity', 'hasStorageRoom']\n",
    "    comfort_score = 0\n",
    "    for feature in comfort_features:\n",
    "        if feature in processed and processed[feature]:\n",
    "            comfort_score += 1\n",
    "    processed['comfort_score'] = comfort_score\n",
    "    \n",
    "    # 5. Remove rarely used or uninformative columns\n",
    "    columns_to_drop = [\n",
    "        'buildYear', 'buildingMaterial', 'ownership', \n",
    "        'schoolDistance', 'clinicDistance', 'kindergartenDistance', \n",
    "        'restaurantDistance', 'collegeDistance', 'pharmacyDistance', 'postOfficeDistance',\n",
    "        'id'  # ID is usually not needed for ML\n",
    "    ]\n",
    "    \n",
    "    for col in columns_to_drop:\n",
    "        if col in processed:\n",
    "            processed = processed.drop(col)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcd80a",
   "metadata": {},
   "source": [
    "Let's create an additional function to normalize numerical features. This will not be part of the main `process_row` function, but can be applied to the data after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83601740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numeric_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes numerical features to the range [0, 1].\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with numerical features\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized features\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # Define numerical columns (excluding boolean and price - the target variable)\n",
    "    numeric_columns = [col for col in df.columns \n",
    "                      if df[col].dtype in ['int64', 'float64'] \n",
    "                      and col != 'price'\n",
    "                      and not (df[col].isin([0, 1]).all() and df[col].nunique() <= 2)]\n",
    "    \n",
    "    # Create a MinMaxScaler instance\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Apply normalization to numerical columns\n",
    "    if numeric_columns:\n",
    "        normalized_df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4eed5",
   "metadata": {},
   "source": [
    "Let's apply `process_row` function to the dataset to test how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106fd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the process_row function to each row of the dataset\n",
    "processed_df = df.apply(process_row, axis=1)\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the structure of the processed dataset\n",
    "processed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for binary features after processing\n",
    "for col in binary_columns:\n",
    "    if col in processed_df.columns:\n",
    "        print(f\"{col}: {processed_df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64d6a3",
   "metadata": {},
   "source": [
    "Let's check how the new features look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new features\n",
    "new_features = ['floor_ratio', 'price_per_m2', 'comfort_score', 'type_numeric', 'condition_numeric']\n",
    "for feature in new_features:\n",
    "    if feature in processed_df.columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        if feature == 'comfort_score':\n",
    "            plt.title(f'Distribution of {feature}')\n",
    "            sns.countplot(x=processed_df[feature])\n",
    "        else:\n",
    "            plt.title(f'Distribution of {feature}')\n",
    "            sns.histplot(processed_df[feature], kde=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c6515",
   "metadata": {},
   "source": [
    "Let's apply normalization to the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization to the processed dataset\n",
    "normalized_df = normalize_numeric_features(processed_df)\n",
    "\n",
    "# Check normalization results\n",
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79526b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the range of normalized features\n",
    "numeric_columns = [col for col in normalized_df.columns \n",
    "                  if normalized_df[col].dtype in ['int64', 'float64'] \n",
    "                  and col != 'price'\n",
    "                  and not (normalized_df[col].isin([0, 1]).all() and normalized_df[col].nunique() <= 2)]\n",
    "\n",
    "print(\"Ranges of normalized features:\")\n",
    "for col in numeric_columns:\n",
    "    print(f\"{col}: [{normalized_df[col].min()}, {normalized_df[col].max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68cd460",
   "metadata": {},
   "source": [
    "Let's check how features correlate with each other and with the target variable (price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55086cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "correlation_matrix = normalized_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by correlation with price\n",
    "price_correlations = correlation_matrix['price'].sort_values(ascending=False)\n",
    "print(\"Correlation of features with price:\")\n",
    "print(price_correlations)\n",
    "\n",
    "# Visualize top-10 features by correlation with price\n",
    "plt.figure(figsize=(12, 6))\n",
    "price_correlations.drop('price').sort_values(ascending=False).head(10).plot(kind='bar')\n",
    "plt.title('Top-10 Features by Correlation with Price')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde61ed4",
   "metadata": {},
   "source": [
    "## FINAL\n",
    "\n",
    "Have been developed the `process_row` function that performs all the necessary data transformations for project:                                                                                         \n",
    "\n",
    "1. Converts binary features from strings ('yes'/'no') to boolean (True/False)\n",
    "2. Fills missing values\n",
    "3. Converts categorical features to numerical\n",
    "4. Creates new informative features\n",
    "5. Removes unnecessary features\n",
    "\n",
    "Also implemented an additional `normalize_numeric_features` function to normalize numerical features.\n",
    "\n",
    "Below is the final version of the `process_row` function that can be used in the Processor component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row_final(row: pd.Series, normalize=True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Final function for processing data rows.\n",
    "    Includes all necessary transformations and optional normalization.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): Row from DataFrame with apartment data\n",
    "        normalize (bool): Flag indicating whether to normalize numerical features\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Processed row with transformed data\n",
    "    \"\"\"\n",
    "    # Create a copy of the row\n",
    "    processed = row.copy()\n",
    "    \n",
    "    # 1. Convert binary string features to boolean values\n",
    "    binary_columns = ['hasParkingSpace', 'hasBalcony', 'hasElevator', 'hasSecurity', 'hasStorageRoom']\n",
    "    for col in binary_columns:\n",
    "        if col in processed:\n",
    "            # Fill missing values\n",
    "            if pd.isna(processed[col]):\n",
    "                processed[col] = 'no'\n",
    "                \n",
    "                # For hasElevator, determine by building type and floor count\n",
    "                if col == 'hasElevator' and 'floorCount' in processed and 'type' in processed:\n",
    "                    if (not pd.isna(processed['floorCount']) and processed['floorCount'] > 4) or \\\n",
    "                       (not pd.isna(processed['type']) and processed['type'] == 'blockOfFlats'):\n",
    "                        processed[col] = 'yes'\n",
    "            \n",
    "            # Convert yes/no to True/False\n",
    "            processed[col] = True if processed[col] == 'yes' else False\n",
    "    \n",
    "    # 2. Fill missing values in numeric columns\n",
    "    numeric_columns = {\n",
    "        'floor': 2,              # Median value\n",
    "        'floorCount': 5,         # Median value\n",
    "        'squareMeters': 50,      # Typical apartment size\n",
    "        'rooms': 2,              # Typical number of rooms\n",
    "        'centreDistance': 5.0,   # Typical distance from center\n",
    "        'poiCount': 10           # Average number of POIs\n",
    "    }\n",
    "    \n",
    "    for col, default_value in numeric_columns.items():\n",
    "        if col in processed and pd.isna(processed[col]):\n",
    "            processed[col] = default_value\n",
    "    \n",
    "    # 3. Convert categorical features to numerical\n",
    "    # Building type\n",
    "    if 'type' in processed:\n",
    "        type_mapping = {\n",
    "            'blockOfFlats': 0,\n",
    "            'tenement': 1,\n",
    "            'apartmentBuilding': 2\n",
    "        }\n",
    "        if not pd.isna(processed['type']):\n",
    "            processed['type_numeric'] = type_mapping.get(processed['type'], 3)\n",
    "        else:\n",
    "            processed['type_numeric'] = 0\n",
    "        \n",
    "        processed = processed.drop('type')\n",
    "    \n",
    "    # Apartment condition\n",
    "    if 'condition' in processed:\n",
    "        condition_mapping = {\n",
    "            'very good': 4,\n",
    "            'good': 3,\n",
    "            'average': 2,\n",
    "            'poor': 1,\n",
    "            'to renovation': 0\n",
    "        }\n",
    "        if not pd.isna(processed['condition']):\n",
    "            processed['condition_numeric'] = condition_mapping.get(processed['condition'], 2)\n",
    "        else:\n",
    "            processed['condition_numeric'] = 2\n",
    "        \n",
    "        processed = processed.drop('condition')\n",
    "    \n",
    "    # City (if present)\n",
    "    if 'city' in processed:\n",
    "        city_mapping = {\n",
    "            'warszawa': 0,\n",
    "            'krakow': 1,\n",
    "            'wroclaw': 2,\n",
    "            'gdansk': 3,\n",
    "            'lodz': 4,\n",
    "            'poznan': 5\n",
    "        }\n",
    "        if not pd.isna(processed['city']):\n",
    "            processed['city_numeric'] = city_mapping.get(processed['city'].lower(), 6)\n",
    "        else:\n",
    "            processed['city_numeric'] = 0\n",
    "        \n",
    "        processed = processed.drop('city')\n",
    "    \n",
    "    # 4. Create new features\n",
    "    # Floor ratio to total floors\n",
    "    if 'floor' in processed and 'floorCount' in processed and processed['floorCount'] > 0:\n",
    "        processed['floor_ratio'] = round(processed['floor'] / processed['floorCount'], 3)\n",
    "    else:\n",
    "        processed['floor_ratio'] = 0.5\n",
    "    \n",
    "    # Price per square meter\n",
    "    if 'price' in processed and 'squareMeters' in processed and processed['squareMeters'] > 0:\n",
    "        processed['price_per_m2'] = round(processed['price'] / processed['squareMeters'], 2)\n",
    "    \n",
    "    # Combined comfort score\n",
    "    comfort_features = ['hasParkingSpace', 'hasBalcony', 'hasElevator', 'hasSecurity', 'hasStorageRoom']\n",
    "    comfort_score = 0\n",
    "    for feature in comfort_features:\n",
    "        if feature in processed and processed[feature]:\n",
    "            comfort_score += 1\n",
    "    processed['comfort_score'] = comfort_score\n",
    "    \n",
    "    # 5. Remove rarely used or uninformative columns\n",
    "    columns_to_drop = [\n",
    "        'buildYear', 'buildingMaterial', 'ownership', \n",
    "        'schoolDistance', 'clinicDistance', 'kindergartenDistance', \n",
    "        'restaurantDistance', 'collegeDistance', 'pharmacyDistance', 'postOfficeDistance',\n",
    "        'id'\n",
    "    ]\n",
    "    \n",
    "    for col in columns_to_drop:\n",
    "        if col in processed:\n",
    "            processed = processed.drop(col)\n",
    "    \n",
    "    # 6. Normalize numerical features (if required)\n",
    "    if normalize:\n",
    "        # Define numerical columns (excluding boolean and price target variable)\n",
    "        numeric_cols = [col for col in processed.index \n",
    "                        if isinstance(processed[col], (int, float)) \n",
    "                        and col != 'price'\n",
    "                        and not (isinstance(processed[col], bool) or (processed[col] in [0, 1] and col in binary_columns))]\n",
    "        \n",
    "        # Normalization using predefined ranges\n",
    "        normalization_ranges = {\n",
    "            'squareMeters': (20, 200),\n",
    "            'rooms': (1, 6),\n",
    "            'floor': (0, 20),\n",
    "            'floorCount': (1, 30),\n",
    "            'centreDistance': (0, 20),\n",
    "            'poiCount': (0, 50),\n",
    "            'type_numeric': (0, 3),\n",
    "            'condition_numeric': (0, 4),\n",
    "            'city_numeric': (0, 6),\n",
    "            'floor_ratio': (0, 1),\n",
    "            'price_per_m2': (20, 500),\n",
    "            'comfort_score': (0, 5)\n",
    "        }\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in normalization_ranges:\n",
    "                min_val, max_val = normalization_ranges[col]\n",
    "                # Limit the value to the range and normalize\n",
    "                val = max(min(processed[col], max_val), min_val)\n",
    "                processed[col] = (val - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb196124",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "For testing the `process_row_final` function refer to the separate notebook `test_processing.ipynb`. This testing notebook includes:\n",
    "\n",
    "1. Complete testing environment with all necessary imports\n",
    "2. Sample dataset with representative data\n",
    "3. Tests with complete and incomplete data\n",
    "4. Visualization of processing results\n",
    "5. Examples of integration with streaming pipeline using RabbitMQ\n",
    "\n",
    "Using a separate test notebook keeps this development notebook focused while providing a clean environment for testing the functions independently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
