{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02daa7ef",
   "metadata": {},
   "source": [
    "# Testing Processing Functions for Apartment Data\n",
    "\n",
    "This notebook serves as a standalone testing environment for the data processing functions developed for the streaming ETL application. It allows testing the processing functions without requiring external data files.\n",
    "\n",
    "## Benefits of this Testing Notebook:\n",
    "\n",
    "1. Test functionality independently from the main application\n",
    "2. Understand how to integrate functions into the streaming ETL pipeline\n",
    "3. Quick start for developers who want to use this processing logic in their own applications\n",
    "\n",
    "## Processing Function Overview\n",
    "\n",
    "The `process_row_final` function performs the following operations:\n",
    "- Converts binary features from strings ('yes'/'no') to boolean values\n",
    "- Fills missing values with sensible defaults\n",
    "- Converts categorical features to numerical values\n",
    "- Creates new features (floor_ratio, price_per_m2, comfort_score)\n",
    "- Removes unnecessary columns\n",
    "- Normalizes numerical features to range [0-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Chart display settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Display all DataFrame columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a26833",
   "metadata": {},
   "source": [
    "## Main Processing Function\n",
    "\n",
    "Below is the implementation of the `process_row_final` function that transforms apartment data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a007cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row_final(row: pd.Series, normalize=True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Final function for processing data rows.\n",
    "    Includes all necessary transformations and optional normalization.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): Row from DataFrame with apartment data\n",
    "        normalize (bool): Flag indicating whether to normalize numerical features\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Processed row with transformed data\n",
    "    \"\"\"\n",
    "    # Create a copy of the row\n",
    "    processed = row.copy()\n",
    "    \n",
    "    # 1. Convert binary string features to boolean values\n",
    "    binary_columns = ['hasParkingSpace', 'hasBalcony', 'hasElevator', 'hasSecurity', 'hasStorageRoom']\n",
    "    for col in binary_columns:\n",
    "        if col in processed:\n",
    "            # Fill missing values\n",
    "            if pd.isna(processed[col]):\n",
    "                processed[col] = 'no'\n",
    "                \n",
    "                # For hasElevator, determine by building type and floor count\n",
    "                if col == 'hasElevator' and 'floorCount' in processed and 'type' in processed:\n",
    "                    if (not pd.isna(processed['floorCount']) and processed['floorCount'] > 4) or \\\n",
    "                       (not pd.isna(processed['type']) and processed['type'] == 'blockOfFlats'):\n",
    "                        processed[col] = 'yes'\n",
    "            \n",
    "            # Convert yes/no to True/False\n",
    "            processed[col] = True if processed[col] == 'yes' else False\n",
    "    \n",
    "    # 2. Fill missing values in numeric columns\n",
    "    numeric_columns = {\n",
    "        'floor': 2,              # Median value\n",
    "        'floorCount': 5,         # Median value\n",
    "        'squareMeters': 50,      # Typical apartment size\n",
    "        'rooms': 2,              # Typical number of rooms\n",
    "        'centreDistance': 5.0,   # Typical distance from center\n",
    "        'poiCount': 10           # Average number of POIs\n",
    "    }\n",
    "    \n",
    "    for col, default_value in numeric_columns.items():\n",
    "        if col in processed and pd.isna(processed[col]):\n",
    "            processed[col] = default_value\n",
    "    \n",
    "    # 3. Convert categorical features to numerical\n",
    "    # Building type\n",
    "    if 'type' in processed:\n",
    "        type_mapping = {\n",
    "            'blockOfFlats': 0,\n",
    "            'tenement': 1,\n",
    "            'apartmentBuilding': 2\n",
    "        }\n",
    "        if not pd.isna(processed['type']):\n",
    "            processed['type_numeric'] = type_mapping.get(processed['type'], 3)\n",
    "        else:\n",
    "            processed['type_numeric'] = 0\n",
    "        \n",
    "        processed = processed.drop('type')\n",
    "    \n",
    "    # Apartment condition\n",
    "    if 'condition' in processed:\n",
    "        condition_mapping = {\n",
    "            'very good': 4,\n",
    "            'good': 3,\n",
    "            'average': 2,\n",
    "            'poor': 1,\n",
    "            'to renovation': 0\n",
    "        }\n",
    "        if not pd.isna(processed['condition']):\n",
    "            processed['condition_numeric'] = condition_mapping.get(processed['condition'], 2)\n",
    "        else:\n",
    "            processed['condition_numeric'] = 2\n",
    "        \n",
    "        processed = processed.drop('condition')\n",
    "    \n",
    "    # City (if present)\n",
    "    if 'city' in processed:\n",
    "        city_mapping = {\n",
    "            'warszawa': 0,\n",
    "            'krakow': 1,\n",
    "            'wroclaw': 2,\n",
    "            'gdansk': 3,\n",
    "            'lodz': 4,\n",
    "            'poznan': 5\n",
    "        }\n",
    "        if not pd.isna(processed['city']):\n",
    "            processed['city_numeric'] = city_mapping.get(processed['city'].lower(), 6)\n",
    "        else:\n",
    "            processed['city_numeric'] = 0\n",
    "        \n",
    "        processed = processed.drop('city')\n",
    "    \n",
    "    # 4. Create new features\n",
    "    # Floor ratio to total floors\n",
    "    if 'floor' in processed and 'floorCount' in processed and processed['floorCount'] > 0:\n",
    "        processed['floor_ratio'] = round(processed['floor'] / processed['floorCount'], 3)\n",
    "    else:\n",
    "        processed['floor_ratio'] = 0.5\n",
    "    \n",
    "    # Price per square meter\n",
    "    if 'price' in processed and 'squareMeters' in processed and processed['squareMeters'] > 0:\n",
    "        processed['price_per_m2'] = round(processed['price'] / processed['squareMeters'], 2)\n",
    "    \n",
    "    # Combined comfort score\n",
    "    comfort_features = ['hasParkingSpace', 'hasBalcony', 'hasElevator', 'hasSecurity', 'hasStorageRoom']\n",
    "    comfort_score = 0\n",
    "    for feature in comfort_features:\n",
    "        if feature in processed and processed[feature]:\n",
    "            comfort_score += 1\n",
    "    processed['comfort_score'] = comfort_score\n",
    "    \n",
    "    # 5. Remove rarely used or uninformative columns\n",
    "    columns_to_drop = [\n",
    "        'buildYear', 'buildingMaterial', 'ownership', \n",
    "        'schoolDistance', 'clinicDistance', 'kindergartenDistance', \n",
    "        'restaurantDistance', 'collegeDistance', 'pharmacyDistance', 'postOfficeDistance',\n",
    "        'id'\n",
    "    ]\n",
    "    \n",
    "    for col in columns_to_drop:\n",
    "        if col in processed:\n",
    "            processed = processed.drop(col)\n",
    "    \n",
    "    # 6. Normalize numerical features (if required)\n",
    "    if normalize:\n",
    "        # Define numerical columns (excluding boolean and price target variable)\n",
    "        numeric_cols = [col for col in processed.index \n",
    "                        if isinstance(processed[col], (int, float)) \n",
    "                        and col != 'price'\n",
    "                        and not (isinstance(processed[col], bool) or (processed[col] in [0, 1] and col in binary_columns))]\n",
    "        \n",
    "        # Normalization using predefined ranges\n",
    "        normalization_ranges = {\n",
    "            'squareMeters': (20, 200),\n",
    "            'rooms': (1, 6),\n",
    "            'floor': (0, 20),\n",
    "            'floorCount': (1, 30),\n",
    "            'centreDistance': (0, 20),\n",
    "            'poiCount': (0, 50),\n",
    "            'type_numeric': (0, 3),\n",
    "            'condition_numeric': (0, 4),\n",
    "            'city_numeric': (0, 6),\n",
    "            'floor_ratio': (0, 1),\n",
    "            'price_per_m2': (20, 500),\n",
    "            'comfort_score': (0, 5)\n",
    "        }\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in normalization_ranges:\n",
    "                min_val, max_val = normalization_ranges[col]\n",
    "                # Limit the value to the range and normalize\n",
    "                val = max(min(processed[col], max_val), min_val)\n",
    "                processed[col] = (val - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce465d",
   "metadata": {},
   "source": [
    "## Test 1: Processing Sample Dataset\n",
    "\n",
    "Let's create a sample dataset and apply processing function to test its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with representative data\n",
    "sample_data = {\n",
    "    'id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'price': [350000, 420000, 280000, 500000, 330000],\n",
    "    'squareMeters': [45, 60, 38, 75, 50],\n",
    "    'rooms': [2, 3, 1, 4, 2],\n",
    "    'floor': [3, 2, 1, 5, 4],\n",
    "    'floorCount': [5, 4, 4, 8, 6],\n",
    "    'hasParkingSpace': ['yes', 'yes', 'no', 'yes', None],\n",
    "    'hasBalcony': ['no', 'yes', 'no', 'yes', 'yes'],\n",
    "    'hasElevator': ['yes', 'no', 'no', 'yes', None],\n",
    "    'hasSecurity': ['no', 'no', 'no', 'yes', 'yes'],\n",
    "    'hasStorageRoom': ['yes', 'no', 'no', 'yes', 'no'],\n",
    "    'centreDistance': [4.5, 6.0, 3.0, 8.5, 5.2],\n",
    "    'poiCount': [12, 8, 15, 7, None],\n",
    "    'type': ['blockOfFlats', 'tenement', 'blockOfFlats', 'apartmentBuilding', None],\n",
    "    'condition': ['good', 'very good', 'to renovation', 'good', 'average'],\n",
    "    'city': ['warszawa', 'krakow', 'warszawa', 'gdansk', 'poznan'],\n",
    "    'buildYear': [2005, 1970, 1990, 2015, None],\n",
    "    'buildingMaterial': ['brick', 'concrete', 'concrete', 'brick', None],\n",
    "    'ownership': ['full ownership', 'cooperative', 'full ownership', 'full ownership', 'cooperative'],\n",
    "    'schoolDistance': [0.8, 1.2, 0.5, 2.0, 1.5],\n",
    "    'clinicDistance': [1.5, 0.7, 1.0, 2.5, 1.8],\n",
    "    'kindergartenDistance': [0.6, 1.0, 0.4, 1.8, 1.2],\n",
    "    'restaurantDistance': [0.3, 0.5, 0.2, 1.0, 0.7],\n",
    "    'collegeDistance': [3.5, 2.0, 4.0, 5.5, 3.0],\n",
    "    'pharmacyDistance': [0.4, 0.8, 0.3, 1.2, 0.9],\n",
    "    'postOfficeDistance': [0.7, 1.5, 0.6, 2.0, 1.2]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the sample data\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Show the sample data\n",
    "print(\"Sample Dataset Created:\")\n",
    "print(f\"Shape: {sample_df.shape}\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the process_row_final function to the sample dataset\n",
    "processed_sample_df = sample_df.apply(process_row_final, axis=1)\n",
    "\n",
    "# Display the processed dataset\n",
    "print(\"Processed Sample Dataset:\")\n",
    "processed_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1ec11",
   "metadata": {},
   "source": [
    "## Test 2: Individual Row Processing\n",
    "\n",
    "Let's process a single row to verify that the function works correctly on individual rows (as it would in the streaming pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad31ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and process a single row to verify individual row processing\n",
    "test_row = sample_df.iloc[2]  # Take the third row\n",
    "print(\"Original row:\")\n",
    "print(test_row)\n",
    "\n",
    "# Process the row with normalization enabled\n",
    "processed_row = process_row_final(test_row, normalize=True)\n",
    "print(\"\\nProcessed row with normalization:\")\n",
    "print(processed_row)\n",
    "\n",
    "# Process the row without normalization to see raw transformations\n",
    "processed_row_no_norm = process_row_final(test_row, normalize=False)\n",
    "print(\"\\nProcessed row without normalization:\")\n",
    "print(processed_row_no_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed85a64",
   "metadata": {},
   "source": [
    "## Test 3: Testing with Incomplete Data\n",
    "\n",
    "Testing the robustness of our function by providing incomplete data - a common scenario in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe412ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with incomplete data to verify robustness\n",
    "incomplete_data = {\n",
    "    'price': [400000],\n",
    "    'squareMeters': [55],\n",
    "    'rooms': [2],\n",
    "    # Missing several fields to test default value handling\n",
    "    'type': ['blockOfFlats']\n",
    "}\n",
    "\n",
    "incomplete_df = pd.DataFrame(incomplete_data)\n",
    "print(\"Incomplete row:\")\n",
    "print(incomplete_df.iloc[0])\n",
    "\n",
    "# Process the incomplete row\n",
    "processed_incomplete = process_row_final(incomplete_df.iloc[0])\n",
    "print(\"\\nProcessed incomplete row:\")\n",
    "print(processed_incomplete)\n",
    "\n",
    "# Check which fields were filled with default values\n",
    "print(\"\\nFields filled with default values:\")\n",
    "default_filled = [col for col in processed_incomplete.index if col not in incomplete_df.columns]\n",
    "for col in default_filled:\n",
    "    print(f\"{col}: {processed_incomplete[col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4da77",
   "metadata": {},
   "source": [
    "Visualize the effect of the processing by looking at the distribution of the generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468455b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of processing on a specific feature\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(y=processed_sample_df['comfort_score'])\n",
    "plt.title('Comfort Score Distribution')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='comfort_score', y='price', data=processed_sample_df)\n",
    "plt.title('Comfort Score vs Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b572ba",
   "metadata": {},
   "source": [
    "## Integration with Streaming ETL Pipeline\n",
    "\n",
    "This section shows how to integrate the `process_row_final` function into a streaming ETL pipeline with RabbitMQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84229c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pika\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Import the process_row_final function from this module\n",
    "# from processing_module import process_row_final\n",
    "\n",
    "# RabbitMQ connection setup\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters(host='rabbitmq'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# Define queues\n",
    "channel.queue_declare(queue='raw_data')\n",
    "channel.queue_declare(queue='processed_data')\n",
    "\n",
    "def callback(ch, method, properties, body):\n",
    "    \"\"\"Process incoming messages from the raw_data queue\"\"\"\n",
    "    try:\n",
    "        # Parse the message body as JSON\n",
    "        message = json.loads(body)\n",
    "        \n",
    "        # Convert the JSON object to a pandas Series\n",
    "        row = pd.Series(message)\n",
    "        \n",
    "        # Process the row using our function\n",
    "        processed_row = process_row_final(row, normalize=True)\n",
    "        \n",
    "        # Convert the processed Series back to a dictionary\n",
    "        processed_dict = processed_row.to_dict()\n",
    "        \n",
    "        # Send the processed data to the next component\n",
    "        channel.basic_publish(\n",
    "            exchange='',\n",
    "            routing_key='processed_data',\n",
    "            body=json.dumps(processed_dict)\n",
    "        )\n",
    "        \n",
    "        # Acknowledge the message\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "        print(f\"Processed message: {message.get('id', 'unknown')}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing message: {e}\")\n",
    "        ch.basic_nack(delivery_tag=method.delivery_tag)\n",
    "\n",
    "# Set up consumer\n",
    "channel.basic_consume(queue='raw_data', on_message_callback=callback)\n",
    "\n",
    "print('Processor started. Waiting for messages...')\n",
    "channel.start_consuming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29578f16",
   "metadata": {},
   "source": [
    "## Test 4: JSON Message Processing Simulation\n",
    "\n",
    "Simulate processing of JSON messages as they would appear in the RabbitMQ message queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of converting between JSON and pandas Series\n",
    "# (which is what would happen in the streaming pipeline)\n",
    "\n",
    "# Sample input JSON that might come from RabbitMQ\n",
    "sample_json = {\n",
    "    \"id\": 1006,\n",
    "    \"price\": 450000,\n",
    "    \"squareMeters\": 65,\n",
    "    \"rooms\": 3,\n",
    "    \"floor\": 4,\n",
    "    \"floorCount\": 7,\n",
    "    \"hasParkingSpace\": \"yes\",\n",
    "    \"hasBalcony\": \"yes\",\n",
    "    \"hasElevator\": \"yes\",\n",
    "    \"hasSecurity\": \"no\",\n",
    "    \"hasStorageRoom\": \"yes\",\n",
    "    \"centreDistance\": 3.2,\n",
    "    \"poiCount\": 18,\n",
    "    \"type\": \"blockOfFlats\",\n",
    "    \"condition\": \"good\",\n",
    "    \"city\": \"warszawa\"\n",
    "}\n",
    "\n",
    "# Convert JSON to pandas Series\n",
    "input_series = pd.Series(sample_json)\n",
    "print(\"Input Series (from JSON):\")\n",
    "print(input_series)\n",
    "\n",
    "# Process the row\n",
    "processed_series = process_row_final(input_series)\n",
    "print(\"\\nProcessed Series:\")\n",
    "print(processed_series)\n",
    "\n",
    "# Convert back to JSON (dict) for sending to the next component\n",
    "output_json = processed_series.to_dict()\n",
    "print(\"\\nOutput JSON (to be sent to next component):\")\n",
    "import json\n",
    "print(json.dumps(output_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a530a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This testing notebook confirms that the `process_row_final` function is working as expected and is ready to be integrated into the streaming ETL pipeline. The function:\n",
    "\n",
    "1. Properly transforms binary fields from 'yes'/'no' to boolean values\n",
    "2. Fills missing values with reasonable defaults\n",
    "3. Converts categorical features to numerical values\n",
    "4. Creates new features (floor_ratio, price_per_m2, comfort_score)\n",
    "5. Removes unnecessary columns\n",
    "6. Normalizes numerical features when requested\n",
    "7. Handles incomplete data robustly\n",
    "8. Can be easily integrated into a streaming ETL pipeline\n",
    "\n",
    "This implementation can be used in the Processor component of the streaming ETL pipeline to prepare apartment data for analysis and machine learning applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
